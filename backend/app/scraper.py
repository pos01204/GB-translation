"""
ì•„ì´ë””ì–´ìŠ¤(Idus) ìƒí’ˆ í¬ë¡¤ë§ ëª¨ë“ˆ
ëª¨ë“  ìƒì„¸ ì´ë¯¸ì§€ ìˆ˜ì§‘ - í•„í„°ë§ ìµœì†Œí™”
"""
import asyncio
import re
import os
from typing import Optional
from playwright.async_api import async_playwright, Browser, Page, BrowserContext
from playwright_stealth import stealth_async

from .models import ProductData, ProductOption


class IdusScraper:
    """ì•„ì´ë””ì–´ìŠ¤ ìƒí’ˆ í˜ì´ì§€ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.playwright = None
        self._initialized = False
        
    async def initialize(self):
        if self._initialized:
            return
            
        print("ğŸ”§ Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì¤‘...")
        
        try:
            self.playwright = await async_playwright().start()
            
            is_docker = os.path.exists('/.dockerenv') or os.getenv('RAILWAY_ENVIRONMENT')
            
            launch_args = [
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-accelerated-2d-canvas',
                '--no-first-run',
                '--no-zygote',
                '--disable-gpu',
            ]
            
            if is_docker:
                launch_args.append('--single-process')
                print("ğŸ³ Docker í™˜ê²½ ê°ì§€ë¨")
            
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=launch_args
            )
            
            self.context = await self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                locale='ko-KR',
            )
            
            self._initialized = True
            print("âœ… Playwright ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Playwright ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
    async def close(self):
        print("ğŸ”§ Playwright ë¸Œë¼ìš°ì € ì¢…ë£Œ ì¤‘...")
        if self.context:
            try: await self.context.close()
            except: pass
        if self.browser:
            try: await self.browser.close()
            except: pass
        if self.playwright:
            try: await self.playwright.stop()
            except: pass
        self._initialized = False
        print("âœ… Playwright ë¸Œë¼ìš°ì € ì¢…ë£Œ ì™„ë£Œ")
    
    async def scrape_product(self, url: str) -> ProductData:
        if not self._initialized:
            await self.initialize()
        
        print(f"ğŸ“„ í¬ë¡¤ë§ ì‹œì‘: {url}")
        
        page = await self.context.new_page()
        await stealth_async(page)
        
        # ë„¤íŠ¸ì›Œí¬ì—ì„œ ì´ë¯¸ì§€ URL ìˆ˜ì§‘
        network_images: set[str] = set()
        
        def on_response(response):
            try:
                resp_url = response.url
                # ì´ë¯¸ì§€ ì‘ë‹µ ë˜ëŠ” Idus ì´ë¯¸ì§€ CDN
                if response.request.resource_type == "image":
                    if resp_url.startswith('http'):
                        network_images.add(resp_url)
                elif 'image.idus.com' in resp_url and resp_url.startswith('http'):
                    network_images.add(resp_url)
            except:
                pass
        
        page.on("response", on_response)
        
        try:
            await page.goto(url, wait_until='networkidle', timeout=60000)
            await asyncio.sleep(3)
            
            # 1. ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = await self._get_title(page)
            artist_name = await self._get_artist(page)
            price = await self._get_price(page)
            description = await self._get_description(page)
            options = await self._get_options(page)
            
            # 2. ì´ë¯¸ì§€ ìˆ˜ì§‘ - ìŠ¤í¬ë¡¤í•˜ë©´ì„œ ëª¨ë“  ì´ë¯¸ì§€ ë¡œë“œ
            print("ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì‹œì‘...")
            await self._scroll_entire_page(page)
            print("ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì™„ë£Œ")
            
            # 3. DOMì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ URL ìˆ˜ì§‘
            dom_images = await self._collect_all_image_urls(page)
            
            # 4. ëª¨ë“  ì´ë¯¸ì§€ í•©ì¹˜ê¸° (ë„¤íŠ¸ì›Œí¬ + DOM)
            all_images = list(network_images) + dom_images
            
            # 5. ìµœì†Œí•œì˜ í•„í„°ë§ë§Œ ì ìš© (ì•„ì´ì½˜/ë¡œê³ ë§Œ ì œì™¸)
            filtered = self._minimal_filter(all_images)
            
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {title}")
            print(f"   - ì‘ê°€: {artist_name}")
            print(f"   - ê°€ê²©: {price}")
            print(f"   - ì˜µì…˜: {len(options)}ê°œ")
            print(f"   - ì´ë¯¸ì§€: {len(filtered)}ê°œ (ë„¤íŠ¸ì›Œí¬: {len(network_images)}, DOM: {len(dom_images)})")
            
            return ProductData(
                url=url,
                title=title,
                artist_name=artist_name,
                price=price,
                description=description,
                options=options,
                detail_images=filtered,
                image_texts=[]
            )
            
        finally:
            await page.close()

    async def _get_title(self, page: Page) -> str:
        try:
            title = await page.title()
            if title:
                clean = title.replace(" | ì•„ì´ë””ì–´ìŠ¤", "").strip()
                if clean and len(clean) >= 3:
                    return clean
        except: pass
        return "ì œëª© ì—†ìŒ"

    async def _get_artist(self, page: Page) -> str:
        try:
            link = await page.query_selector('a[href*="/artist/"]')
            if link:
                text = (await link.inner_text() or "").strip()
                if 2 <= len(text) <= 50:
                    return text
        except: pass
        return "ì‘ê°€ëª… ì—†ìŒ"

    async def _get_price(self, page: Page) -> str:
        try:
            result = await page.evaluate("""
                () => {
                    const els = document.querySelectorAll('[class*="price"], [class*="Price"]');
                    for (const el of els) {
                        const m = (el.innerText || '').match(/[\\d,]+\\s*ì›/);
                        if (m) return m[0];
                    }
                    return null;
                }
            """)
            if result: return result
        except: pass
        return "ê°€ê²© ì •ë³´ ì—†ìŒ"

    async def _get_description(self, page: Page) -> str:
        # íƒ­ í´ë¦­ ì‹œë„
        try:
            for sel in ['text="ì‘í’ˆì •ë³´"', 'text="ìƒí’ˆì •ë³´"', 'text="ìƒì„¸ì •ë³´"']:
                tab = await page.query_selector(sel)
                if tab:
                    await tab.click()
                    await asyncio.sleep(1)
                    break
        except: pass
        
        try:
            text = await page.evaluate("""
                () => {
                    const selectors = ['article', '[class*="detail"]', '[class*="description"]', '[class*="content"]', 'main'];
                    let longest = '';
                    for (const sel of selectors) {
                        document.querySelectorAll(sel).forEach(el => {
                            const t = el.innerText || '';
                            if (t.length > longest.length && t.length > 100) {
                                // ë…¸ì´ì¦ˆ í•„í„°
                                if (!t.includes('ë¡œê·¸ì¸') && !t.includes('ì¥ë°”êµ¬ë‹ˆ')) {
                                    longest = t;
                                }
                            }
                        });
                    }
                    return longest || null;
                }
            """)
            if text:
                return text[:6000]
        except: pass
        return "ì„¤ëª… ì—†ìŒ"

    async def _get_options(self, page: Page) -> list[ProductOption]:
        options_dict: dict[str, set[str]] = {}
        
        try:
            # í›„ê¸°ì—ì„œ ì˜µì…˜ ì¶”ì¶œ
            texts = await page.evaluate("""
                () => {
                    const result = [];
                    document.querySelectorAll('a, span, div, p').forEach(el => {
                        const t = el.innerText || '';
                        if (t.includes('êµ¬ë§¤ì‘í’ˆ') && t.includes(':')) result.push(t);
                    });
                    return result;
                }
            """)
            
            for text in texts:
                for part in text.split("êµ¬ë§¤ì‘í’ˆ")[1:]:
                    for opt in part.split("*"):
                        match = re.search(r':\s*([^:]+):\s*([^*]+)', opt)
                        if match:
                            name = match.group(1).strip()
                            value = match.group(2).strip()
                            if name and value:
                                options_dict.setdefault(name, set()).add(value)
        except: pass
        
        return [ProductOption(name=n, values=list(v)) for n, v in options_dict.items() if v]

    async def _scroll_entire_page(self, page: Page):
        """í˜ì´ì§€ ì „ì²´ë¥¼ ì²œì²œíˆ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  lazy-load ì´ë¯¸ì§€ ë¡œë“œ"""
        try:
            # ì´ í˜ì´ì§€ ë†’ì´
            total = await page.evaluate("document.body.scrollHeight")
            current = 0
            step = 300  # 300pxì”© ìŠ¤í¬ë¡¤
            
            while current < total:
                await page.evaluate(f"window.scrollTo(0, {current})")
                await asyncio.sleep(0.25)  # ì´ë¯¸ì§€ ë¡œë“œ ëŒ€ê¸°
                current += step
                
                # ë™ì  ì½˜í…ì¸ ë¡œ ë†’ì´ê°€ ëŠ˜ì–´ë‚¬ëŠ”ì§€ í™•ì¸
                new_total = await page.evaluate("document.body.scrollHeight")
                if new_total > total:
                    total = new_total
            
            # ë§¨ ì•„ë˜ì—ì„œ ì ì‹œ ëŒ€ê¸° (ë§ˆì§€ë§‰ ì´ë¯¸ì§€ ë¡œë“œ)
            await asyncio.sleep(1.5)
            
            # ë§¨ ìœ„ë¡œ ëŒì•„ê°€ê¸°
            await page.evaluate("window.scrollTo(0, 0)")
            await asyncio.sleep(0.5)
            
        except Exception as e:
            print(f"ìŠ¤í¬ë¡¤ ì˜¤ë¥˜: {e}")

    async def _collect_all_image_urls(self, page: Page) -> list[str]:
        """DOMì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ URL ìˆ˜ì§‘"""
        try:
            urls = await page.evaluate("""
                () => {
                    const urls = [];
                    
                    // 1. img íƒœê·¸
                    document.querySelectorAll('img').forEach(img => {
                        if (img.src && img.src.startsWith('http')) urls.push(img.src);
                        
                        // data-* ì†ì„±
                        ['data-src', 'data-original', 'data-lazy-src', 'data-url'].forEach(attr => {
                            const val = img.getAttribute(attr);
                            if (val && val.startsWith('http')) urls.push(val);
                        });
                        
                        // srcset
                        const srcset = img.getAttribute('srcset');
                        if (srcset) {
                            srcset.split(',').forEach(part => {
                                const url = part.trim().split(' ')[0];
                                if (url && url.startsWith('http')) urls.push(url);
                            });
                        }
                    });
                    
                    // 2. source íƒœê·¸
                    document.querySelectorAll('source').forEach(src => {
                        const srcset = src.getAttribute('srcset');
                        if (srcset) {
                            srcset.split(',').forEach(part => {
                                const url = part.trim().split(' ')[0];
                                if (url && url.startsWith('http')) urls.push(url);
                            });
                        }
                    });
                    
                    // 3. background-image
                    document.querySelectorAll('*').forEach(el => {
                        try {
                            const bg = getComputedStyle(el).backgroundImage;
                            if (bg && bg !== 'none') {
                                const match = bg.match(/url\\(['"]?(https?:\\/\\/[^'"\\)]+)['"]?\\)/);
                                if (match) urls.push(match[1]);
                            }
                        } catch(e) {}
                    });
                    
                    // 4. a íƒœê·¸ì˜ href (ì´ë¯¸ì§€ ë§í¬)
                    document.querySelectorAll('a[href]').forEach(a => {
                        const href = a.getAttribute('href');
                        if (href && /\\.(jpg|jpeg|png|webp|gif)(\\?|$)/i.test(href)) {
                            if (href.startsWith('http')) urls.push(href);
                        }
                    });
                    
                    return urls;
                }
            """)
            return urls or []
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return []

    def _minimal_filter(self, images: list[str]) -> list[str]:
        """
        ìµœì†Œí•œì˜ í•„í„°ë§ - ëª…ë°±í•œ ì•„ì´ì½˜/ë¡œê³ ë§Œ ì œì™¸
        ì¤‘ë³µ ì œê±°ëŠ” ì •í™•í•œ URL ê¸°ì¤€ìœ¼ë¡œë§Œ
        """
        # í™•ì‹¤íˆ ì œì™¸í•  íŒ¨í„´ë§Œ (ë§¤ìš° ë³´ìˆ˜ì )
        exclude = [
            '/icon', '/sprite', '/logo', '/avatar', '/badge', '/emoji',
            '/button', '/arrow', '/check/', '/close/', '/menu/', '/search/',
            'facebook.', 'twitter.', 'instagram.', 'kakao.', 'naver.',
            'google.com', 'apple.com', 'play.google',
            '/qr', '/escrow', '/membership',
            'data:image',  # base64 ì¸ë¼ì¸ ì´ë¯¸ì§€
        ]
        
        result = []
        seen = set()  # ì •í™•í•œ URL ì¤‘ë³µ ì œê±°
        
        for img in images:
            if not img or not img.startswith('http'):
                continue
            
            # ì •í™•í•œ URL ì¤‘ë³µ ì²´í¬
            if img in seen:
                continue
            seen.add(img)
            
            low = img.lower()
            
            # SVG ì œì™¸
            if '.svg' in low:
                continue
            
            # ëª…ë°±í•œ ì œì™¸ íŒ¨í„´ë§Œ ì²´í¬
            skip = False
            for pattern in exclude:
                if pattern in low:
                    skip = True
                    break
            if skip:
                continue
            
            # ì´ë¯¸ì§€ í™•ì¥ì ë˜ëŠ” Idus CDNì´ë©´ í¬í•¨
            is_image = any(ext in low for ext in ['.jpg', '.jpeg', '.png', '.webp', '.gif'])
            is_idus = 'idus.com' in low
            
            if is_image or is_idus:
                result.append(img)
        
        print(f"ğŸ“· í•„í„°ë§: {len(images)}ê°œ â†’ {len(result)}ê°œ")
        return result[:150]  # ìµœëŒ€ 150ê°œ


if __name__ == "__main__":
    async def test():
        scraper = IdusScraper()
        await scraper.initialize()
        try:
            result = await scraper.scrape_product(
                "https://www.idus.com/v2/product/87beb859-49b2-4c18-86b4-f300b31d6247"
            )
            print(f"\nì œëª©: {result.title}")
            print(f"ì´ë¯¸ì§€: {len(result.detail_images)}ê°œ")
            for i, img in enumerate(result.detail_images[:10]):
                print(f"  {i+1}. {img[:80]}...")
        finally:
            await scraper.close()
    
    asyncio.run(test())
